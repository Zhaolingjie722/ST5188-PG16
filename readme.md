11:49:44.294: [st5188pg16] git -c credential.helper= -c core.quotepath=false -c log.showSignature=false push --progress
--porcelain origin refs/heads/master:master
fatal: unable to access 'https://github.com/WhyCitrus/st5188pg16_deepfake-speech-recognition.git/': Failed to connect to
github.com port 443 after 21053 ms: Could not connect to server
fatal: unable to access 'https://github.com/WhyCitrus/st5188pg16_deepfake-speech-recognition.git/': Failed to connect to
github.com port 443 after 21053 ms: Could not connect to server# DeepFake Recognition for CelebritiesðŸš€ï¸

This is the code repository for deep fake audio recognition of st5188 PG16.

## Environment Configuration

=== python version===

3.10.16 | packaged by conda-forge

=== CPU info ===

Graphics Optimized: G2-standard-32 32vCPUs 16core,128GB memory

=== platform info ===

Machine: x86_64

Platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31

=== GPU info ===

NVIDIA L4 * 2

=== env.yml ===

environment.yml

# Data Description

## In-the-Wild

(1) Source: A real-life deepfake speech dataset compiled by the Fraunhofer AISEC CST team.
The paper Does Audio Deepfake Detection Generalize? (Interspeech 2022) and the official website provide downloads.

(2) Structure: A total of 58 public figures, each with both real and fake audio; 20.8 hours of real audio, 17.2 hours of
fake audio, a total of about 38 hours.

All data is stored in the release_in_the_wild folder. We provide a meta file that records the speaker and label
corresponding to each wav file.

(3) Purpose:

â‘  As the final external test set, measure the cross-domain performance of the model

â‘¡ To test the false positive rate of final model under noise and compression distortion in social media/news clips;

## WaveFake

(1) Source: WaveFake: A Data Set to Facilitate Audio Deepfake Detection (Zenodo / GitHub), NeurIPS 2021 Datasets &
Benchmarks Track, SysSec Group, Ruhr University Bochum.

(2) Structure: 10 subsets, covering 7 TTS networks (WaveGlow, MelGAN, Parallel WaveGAN, etc.).
Each subset contains the corresponding fake audio (â‰ˆ 58 h) of the original LJSpeech (about 6 h) dataset, with a total
duration of > 60 h. The files here are all fraud samples, and are stored in the corresponding folders of their
respective generated models.

(3) Purpose:

â‘  Backbone training set - provides multi-architecture synthetic samples to prevent the model from overfitting to a
single generator;

â‘¡ Baseline data for frequency domain analysis experiments, supporting high-frequency difference visualization;

## Diffusion-based dataset

(1) Source: Diffuse or Confuse project of Brno University of Technology (arXiv 2024â€‘10).

(2) Structure: 14 sets of "pseudo-LJSpeech" are copied based on LJSpeech, and the synthesizers cover Gradâ€‘TTS, DiffWave,
WaveGrad2, DiffGANâ€‘TTS, etc. A total of 183 400 fake recordings, of which 131 000 are generated by the diffusion model;
the total length is about 336 h. Each set is aligned 1â€‘toâ€‘1 with LJSpeech, and CSV (path, synthesizer label, quality
indicators) and original transcription are provided. After import, data_source=diffusion-based, label=0 (fake),
synthetic_model=<name> for model interpretability research.

(3) Purpose:

â‘  Evaluate the attack and defense threats of diffuse TTS to existing detectors

â‘¡ Use it as incremental training data to improve the robustness of the model to the latest generation paradigm.

## voxceleb1

(1) Source: VoxCeleb large-scale speaker recognition dataset released by the Oxford VGG group (INTERSPEECH 2017, 2018;
download from the official website).
Robotics Science and Engineering

(2) Structure: VoxCeleb1: 1 251 celebrities, â‰ˆ 153 k sentences, â‰ˆ 352 h

The audio is a YouTube interview clip with a frame rate of 16 kHz. Projects are grouped in folders with speaker=<id>,
all with label=1.

(3) Purpose:

â‘  Provide large-scale real speech priors for deep fake detection models for comparative learning/self-supervised
pre-training;

â‘¡ Sampling real audio from noisy environments and across channels to improve the model's noise resistance.

## LJSpeech-1.1

(1) Source: Public domain English reading dataset released by Keith Ito & Linda Johnson (13,100 records, single female
voice).

(2) Structure: 44.1 kHz MP3 â†’ resampled to 16 kHz WAV for this project; 1â€“10 s per record, total duration about 24 h.
Supporting metadata.csv (filename|text). In the internal CSV, data_source=ljspeech, speaker=LJ, label=1.

(3) Purpose:

â‘  As a clean real person reference: one-to-one alignment with multiple fake versions (WaveFake, Diffusionâ€‘TTS) during
training/evaluation;

â‘¡ Generate the basis of synthetic speech to ensure that the real and fake content are completely matched, so that the
detector only focuses on acoustic artifacts;

# Data Split

### Overall scale and distribution

1 meta_final.csv contains 499,778 records, with fields including data_source | speaker | absolute_path | relative_path |
label | Set.

2 Divided by Set: train 368,780 (73.8%), test 111,589 (22.3%), inâ€‘test 19,409 (3.9%).

3 Labels are defined as 1=bonafide (real), 0=spoof (fake); the true and false ratios of the three groups are similar (
train 37.9% real, test 35.2%, inâ€‘test 37.8%), and the overall category is not unbalanced.

### Speaker Split

1 train contains 1,050 speakers and test contains 280 speakers, with no overlap (0/1,050).

2 in-test contains 1,029 speakers, of which 1,029 speakers appear in train and have no overlap with test.

3 Advantages: external evaluation (test) does not contain training speakers, which can effectively test cross-speaker
generalization.

### Generative Model Split

Non-diffusion model (classic TTS/GAN/Flow) partitioning strategy

1 Training set + in-test: 13 models appear at the same time, used to learn mainstream non-diffusion forgery features

2 Retain exclusive models for the test set:
(1) ljspeech_hifiGAN
(2) ljspeech_waveglow

These 2 models are completely absent in the training phase, and only 26 200 samples are given in the test to test the
extrapolation performance of the "unseen non-diffusion algorithm"

3 in-test only overlaps with the train model, and its role is to quickly debug and analyze false positives during
training, and does not participate in the final indicators

Diffusion model partitioning strategy

1 Training set + in-test: 6 diffusion models appear at the same time
â€¢ DiffGAN-TTS_naive â€¢ DiffGAN-TTS_shallow â€¢ ProDiff â€¢ tacotron2-DCA_diffwave â€¢ tacotron2-DCA_wavegrad â€¢ wavegrad2

2 Diffusion models exclusively for the test set:
(1) DiffGAN-TTS_aux(2) Grad-TTS(3) NATSpeech_DiffSpeech
39,300 samples in total, specifically to examine the generalization ability of the detector to "unseen diffusion
families"

## Reproduction Instructions

1. run bash setup in terminal ro create a new python environment for following training
2. run Download.ipynb to download all dataset we need and the data will be ranged as shown in directory structure
3. run Exploratory data analysis.ipynb and clustering.ipynb to do some basic analysis
4. run data 11lab.py and diffgan.py to create self-gen test dataset
5. run Data Split.ipynb to implement data set division and generate meta_final.csv files corresponding to training data
6. run mfcc extraction.ipynb to get overall mfcc features for conformer input,you will get a npy folder
7. run 11labtransformer.ipynb to transform raw mp3 data to wav file
8. run CNNBiLSTMClassifier&ConformerClassifier&WhisperClassifier.ipynb to train model and complete model evaluation
9. bash run predict.py to detect target audio file is spoofed or not
   python predict.py
   --input /path to target file or dir which you want to predict
   --model /path to your saved model checkpoint file(.pt) #---note that the parameters of the model should be consistent
   with the file
   --csv /path to save prediction result
   --threshold /Threshold of positive and negative samples

## Directory Structure

- **Data/**: Contains the meta file corresponding to the original data, and all data sets will be downloaded to this
  folder later. We provide a specific data download notebook
- **Notebooks/**: Includes dataset download, dataset partitioning, exploratory data experimental analysis, Spectrogram
  feature batch extraction,test set data conversion
- **Train/**: The core source code includes data loading, data enhancement, model architecture, training and model
  evaluation, and notebooks for batch prediction.
- Train/: Contains data generation pipeline via gan model and 11lab api.
- **src/**: Configuration file, which records the corresponding training environment parameters, including Python
  version, operating system version, cpu info and gpu info
- **environment.yml**: Environment dependency files ensure code reproducibility.

```
DeepFake4Celeb/
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ diffusion_dataset/        # Dataset from mutilple diffusion-based generative model
â”‚   â”œâ”€â”€ generated_audio/          # WaveFake dataset
â”‚   â”œâ”€â”€ inthewild/                # In-the-wild dataset
â”‚   â”œâ”€â”€ LJSpeech-1.1/             # LJSpeech  dataset
â”‚   â”œâ”€â”€ vox1_dev_wav/             # VoxCeleb1 dataset
â”‚   â”œâ”€â”€ vox1_test_wav/            # VoxCeleb1 dataset
â”‚   â””â”€â”€ README.md                 # Explain the source and organization of data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Download.ipynb            # Download all dataset we will used in training and evaluation part
â”‚   â”œâ”€â”€ Exploratory data analysis.ipynb            # Exploratory data analysis, mainly including audio distribution, speaker distribution, audio visualization
â”‚   â”œâ”€â”€ Data Split.ipynb          # Data set division, group processing speakers, avoid data leakage
â”‚   â”œâ”€â”€ clustering.ipynb          # Visual analysis of audio data via pca and t-sne
â”‚   â”œâ”€â”€ mfcc extraction.ipynb     # Data pipeline from raw audio to mfcc features
â”‚   â””â”€â”€ 11lab transformer.ipynb   # Convert mp3 format audio to wav and generate csv file for batch prediction
â”‚
â”œâ”€â”€ data generation/
â”‚   â”œâ”€â”€ 11lab.py                  # generate fake audio samples via 11lab api
â”‚   â”œâ”€â”€ diffgan.py                # generate fake audio samples via pre-trained diff-gan modelâ”‚
â”‚   â”œâ”€â”€ speech.txt                # raw context for 11lab data generation
â”‚   â”œâ”€â”€ text.txt                  # raw context for diff-gan data generation
â”‚   â””â”€â”€ reader_ids.txt            # 11lab api account info
â”‚
â”œâ”€â”€ Train/
â”‚   â”œâ”€â”€ CNNBiLSTMClassifier.ipynb  # The complete training code of CNNBiLSTMClassifier, including the entire pipeline of data loading, enhancement, training, evaluation, and prediction
â”‚   â”œâ”€â”€ ConformerClassifier.ipynb  # The complete training code of ConformerClassifier, including the entire pipeline of data loading, enhancement, training, evaluation, and prediction
â”‚   â””â”€â”€ WhisperClassifier.ipynb    # The complete training code of WhisperClassifier, including the entire pipeline of data loading, enhancement, training, evaluation, and prediction
â”‚
â”œâ”€â”€ batch_predict/
â”‚   â”œâ”€â”€ functions.py                # contains model architecture and data_process funcitons
â”‚   â””â”€â”€ predict.py                  # predict script for any audio u want to detect
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bash setup                # Configure the training environment through environment.yml
â”‚   â””â”€â”€ env info                  # Records the CPU GPU PYTHON version, operating system version and other parameters of the training environment
â”‚
â”œâ”€â”€ environment.yml               # conda Configure file
â”œâ”€â”€ README.md                     # Overall description of the warehouse (project overview, usage, etc.)
â””â”€â”€ .gitignore                    # Git ignore configuration
```

## Dataset source

1. In-the-Wild       : https://deepfake-total.com/in_the_wild
   or https://www.kaggle.com/datasets/abdallamohamed312/in-the-wild-dataset
2. WaveFake          : https://opendatalab.com/OpenDataLab/WaveFake
   or https://www.kaggle.com/datasets/walimuhammadahmad/fakeaudio
3. VoxCeleb1         : https://www.kaggle.com/datasets/namhocayai/voxceleb1/data
4. Diffusion-based   : https://github.com/AntonFirc/diffusion-deepfake-speech-dataset/
5. LJSpeech-1.1      : https://www.kaggle.com/datasets/mathurinache/the-lj-speech-dataset

## References

1. WaveFake: A Data Set to Facilitate Audio Deepfake
   Detection  https://arxiv.org/pdf/2111.02813  https://github.com/rub-syssec/wavefake
2. Diffusion-based: Diffuse or Confuse: A Diffusion Deepfake Speech
   Dataset      https://arxiv.org/abs/2410.06796  https://github.com/AntonFirc/diffusion-deepfake-speech-dataset/
3. VoxCeleb: A large-scale speaker identification
   dataset       https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf
4. In-the-Wild: Does Audio Deepfake Detection Generalize?       https://arxiv.org/pdf/2203.16263
5. LJSpeech-1.1: https://paperswithcode.com/dataset/ljspeech
